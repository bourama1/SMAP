<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns:z="http://www.zotero.org/namespaces/export#"
 xmlns:dcterms="http://purl.org/dc/terms/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:bib="http://purl.org/net/biblio#"
 xmlns:vcard="http://nwalsh.com/rdf/vCard#"
 xmlns:foaf="http://xmlns.com/foaf/0.1/"
 xmlns:prism="http://prismstandard.org/namespaces/1.2/basic/">
    <bib:BookSection rdf:about="urn:isbn:9789811686153">
        <z:itemType>bookSection</z:itemType>
        <dcterms:isPartOf>
            <bib:Book>
                <dcterms:isPartOf>
                    <bib:Series>
                        <dc:title>Machine Learning: Foundations, Methodologies, and Applications</dc:title>
                    </bib:Series>
                </dcterms:isPartOf>
                <dc:title>Artificial Intelligence with Python</dc:title>
                <dc:identifier>ISBN 9789811686153</dc:identifier>
            </bib:Book>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Singapore</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Springer</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Teoh</foaf:surname>
                        <foaf:givenName>Teik Toe</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rong</foaf:surname>
                        <foaf:givenName>Zheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Teoh</foaf:surname>
                        <foaf:givenName>Teik Toe</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rong</foaf:surname>
                        <foaf:givenName>Zheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dc:title>Artificial Intelligence with Python</dc:title>
        <dcterms:abstract>Convolutional Neural Networks are neural networks with convolution layers which perform operations similar to image processing filters. Convolutional Neural Networks are applied in a variety of tasks related to images such as image classification, object detection, and semantic segmentation. Popular Network architectures include ResNet, GoogleNet, and VGG. These networks are often trained on very large datasets, can be downloaded in Keras and Tensorflow, and can be later used for finetuning on other tasks.</dcterms:abstract>
        <dc:date>2022</dc:date>
        <bib:pages>261-275</bib:pages>
        <z:language>en</z:language>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1007/978-981-16-8615-3_16</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-12-16 17:48:30</dcterms:dateSubmitted>
        <z:libraryCatalog>Springer Link</z:libraryCatalog>
        <dc:description>DOI: 10.1007/978-981-16-8615-3_16</dc:description>
    </bib:BookSection>
    <bib:BookSection rdf:about="urn:isbn:9781484227664">
        <z:itemType>bookSection</z:itemType>
        <dcterms:isPartOf>
            <bib:Book>
                <dc:title>Deep Learning with Python: A Hands-on Introduction</dc:title>
                <dc:identifier>ISBN 9781484227664</dc:identifier>
            </bib:Book>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Berkeley, CA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Apress</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ketkar</foaf:surname>
                        <foaf:givenName>Nikhil</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ketkar</foaf:surname>
                        <foaf:givenName>Nikhil</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Convolution Operation</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Convolutional Neural Network</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Deep Learning</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Multiple Kernel</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Neural Network</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>Deep Learning with Python</dc:title>
        <dcterms:abstract>Convolution Neural Networks (CNNs) in essence are neural networks that employ the convolution operation (instead of a fully connected layer) as one of its layers.</dcterms:abstract>
        <dc:date>2017</dc:date>
        <bib:pages>63-78</bib:pages>
        <z:language>en</z:language>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1007/978-1-4842-2766-4_5</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-12-16 17:46:40</dcterms:dateSubmitted>
        <z:libraryCatalog>Springer Link</z:libraryCatalog>
        <dc:description>DOI: 10.1007/978-1-4842-2766-4_5</dc:description>
    </bib:BookSection>
    <bib:BookSection rdf:about="urn:isbn:9781484235164">
        <z:itemType>bookSection</z:itemType>
        <dcterms:isPartOf>
            <bib:Book>
                <dc:title>Deep Learning with Applications Using Python : Chatbots and Face, Object, and Speech Recognition With TensorFlow and Keras</dc:title>
                <dc:identifier>ISBN 9781484235164</dc:identifier>
            </bib:Book>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Berkeley, CA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Apress</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Manaswi</foaf:surname>
                        <foaf:givenName>Navin Kumar</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Manaswi</foaf:surname>
                        <foaf:givenName>Navin Kumar</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Vision Problems</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Convolutional Neural Network (CNN)</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Image Classification Challenge</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Layer Feed-forward Artificial Neural Network</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Natural Language Processing Problems</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Deep Learning with Applications Using Python</dc:title>
        <dcterms:abstract>A convolutional neural network (CNN) is a deep, feed-forward artificial neural network in which the neural network preserves the hierarchical structure by learning internal feature representations and generalizing the features in the common image problems such as object recognition and other computer vision problems. It is not restricted to images; it also receives state-of-the-art results in natural language processing problems and speech recognition.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <bib:pages>91-96</bib:pages>
        <z:language>en</z:language>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1007/978-1-4842-3516-4_6</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-12-16 17:41:35</dcterms:dateSubmitted>
        <z:libraryCatalog>Springer Link</z:libraryCatalog>
        <dc:description>DOI: 10.1007/978-1-4842-3516-4_6</dc:description>
    </bib:BookSection>
    <bib:Article rdf:about="https://ieeexplore.ieee.org/document/7968387">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:identifier>DOI 10.1109/TPAMI.2017.2723009</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Bolei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lapedriza</foaf:surname>
                        <foaf:givenName>Agata</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khosla</foaf:surname>
                        <foaf:givenName>Aditya</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Oliva</foaf:surname>
                        <foaf:givenName>Aude</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Torralba</foaf:surname>
                        <foaf:givenName>Antonio</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Places: A 10 Million Image Database for Scene Recognition | IEEE Journals &amp; Magazine | IEEE Xplore</dc:title>
        <dc:date>2018</dc:date>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://ieeexplore.ieee.org/document/7968387</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-12-09 11:53:44</dcterms:dateSubmitted>
    </bib:Article>
    <rdf:Description rdf:about="https://papers.ssrn.com/abstract=3852766">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Rochester, NY</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Thakurdesai</foaf:surname>
                        <foaf:givenName>Shalva</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vira</foaf:surname>
                        <foaf:givenName>Shubham</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kanitkar</foaf:surname>
                        <foaf:givenName>Gouri</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Save</foaf:surname>
                        <foaf:givenName>Jagruti</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Google vision</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Image annotation</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Image processing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Image query</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Label detection</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>OCR</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>Smart Gallery using Google Vision</dc:title>
        <dcterms:abstract>The proposed project comprises a system which helps intelligent gallery management. For this, a server, mobile interface and web interface has been made. The mobile interface uploads photos to the server and ensures privacy of photos. The server processes the photos and saves the data like size, object analysis and OCR in the database. The web interface let's user login and query the photos for different objects and OCR. The user also has option to sort photos by size or date of upload. If the user wants to clear the gallery, all the images can be deleted or downloaded at once. The user can also search for similar images by giving an image as a query alongside text queries. Technologies used include HTML, CSS, SQL, flutter ensures cross platform compatibility and PHP makes API calls and manages the backend. Google vision ensures reliable image processing.</dcterms:abstract>
        <z:type>SSRN Scholarly Paper</z:type>
        <prism:number>3852766</prism:number>
        <dc:date>2021-05-25</dc:date>
        <dc:identifier>DOI 10.2139/ssrn.3852766</dc:identifier>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://papers.ssrn.com/abstract=3852766</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-12-09 11:46:25</dcterms:dateSubmitted>
        <z:language>en</z:language>
        <z:libraryCatalog>Social Science Research Network</z:libraryCatalog>
    </rdf:Description>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/S2212571X20301347">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2212-571X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Renwu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luo</foaf:surname>
                        <foaf:givenName>Jiaqi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Songshan (Sam)</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Artificial intelligence</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Deep learning</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Destination management</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Photo identification</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Smart tourism</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>Developing an artificial intelligence framework for online destination image photos identification</dc:title>
        <dcterms:abstract>With the development of advanced technologies in computer science, such as deep learning and transfer learning, the tourism field is facing a more intelligent and automated future development environment. In this study, an artificial intelligence (AI) framework is developed to identify tourism photos without human interaction. Adopting online destination photos of Australia as a data source, the results show that the model combining a deep convolutional neural network and mixed transfer learning achieved the best image identification performance. This study identified 25 image classification categories covering all the tourism scenes to serve as a foundation for future tourism computer vision research. The results indicate that the AI photo identification framework is of great benefit for the understanding of projected destination images and enhancing tourism experiences. This study contributes to the existing literature by introducing an intelligent automation framework to big data research in the tourism field, as well as by advancing innovative methodologies of online destination image analysis. Practically, the proposed framework contributes to the marketing and management of smart destinations by offering a state-of-the-art data mining method.</dcterms:abstract>
        <bib:pages>100512</bib:pages>
        <dc:date>2020-12-01</dc:date>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S2212571X20301347</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-12-09 11:46:06</dcterms:dateSubmitted>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2212-571X">
        <dc:title>Journal of Destination Marketing &amp; Management</dc:title>
        <prism:volume>18</prism:volume>
        <dcterms:alternative>Journal of Destination Marketing &amp; Management</dcterms:alternative>
        <dc:identifier>DOI 10.1016/j.jdmm.2020.100512</dc:identifier>
        <dc:identifier>ISSN 2212-571X</dc:identifier>
    </bib:Journal>
    <bib:Article rdf:about="https://www.pnrjournal.com/index.php/home/article/view/8539">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2229-7723"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sharma</foaf:surname>
                        <foaf:givenName>Ujjwal</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Goel</foaf:surname>
                        <foaf:givenName>Tanya</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Singh</foaf:surname>
                        <foaf:givenName>Dr Jagbeer</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Real-Time Image Processing Using Deep Learning With Opencv And Python</dc:title>
        <dcterms:abstract>The observation of laptop imaginative and prescient aids in the improvement of techniques for figuring out presentations and pictures. It contains a variety of functions, including picture recognition, object identification and image production among others. Face recognition, vehicle recognition, online photos, and safety systems all employ object detection. The goal is to identify things using the You Only Look Once (YOLO) technique. When compared to previous object identification algorithms, our method focuses on a few key areas. Unlike other algorithms, YOLO scans the whole photograph through estimating bounding containers the use of convolutional networks and sophistication possibilities for those containers. This permits YOLO to understand an photograph extra fast than different algorithms together with convolutional neural networks and speedy convolutional neural network. By using dependencies like OpenCV, we can identify each object in an image based on the region object in a distinct rectangular box, identify every item and assign its tag to the item the use of those strategies and algorithms primarily based totally on deep learning, which is likewise primarily based totally on system learning. It moreover consists of the nuances of every item-marking strategy.</dcterms:abstract>
        <bib:pages>1905-1908</bib:pages>
        <dc:date>2023-02-11</dc:date>
        <z:language>en</z:language>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.pnrjournal.com/index.php/home/article/view/8539</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-12-09 11:45:22</dcterms:dateSubmitted>
        <z:libraryCatalog>www.pnrjournal.com</z:libraryCatalog>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2229-7723">
        <dc:title>Journal of Pharmaceutical Negative Results</dc:title>
        <dc:identifier>DOI 10.47750/pnr.2023.14.03.246</dc:identifier>
        <dc:identifier>ISSN 2229-7723</dc:identifier>
    </bib:Journal>
    <rdf:Description rdf:about="https://www.semanticscholar.org/paper/Object-Detection-using-Convolutional-Neural-Network-Emmanuel-Onuodu/c61d07cdb15155c9b9fea4af42a3e917f3962a88">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Emmanuel</foaf:surname>
                        <foaf:givenName>Seetam</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Onuodu</foaf:surname>
                        <foaf:givenName>F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Object Detection using Convolutional Neural Network Transfer Learning</dc:title>
        <dcterms:abstract>Any machine learning algorithm's ability to extract salient (relevant) characteristics is critical to its success. Traditional machine learning methods rely on domain expert-generated input features or computational feature extraction techniques. A Convolutional Neural Network (CNN) is a type of artificial intelligence inspired by how the human brain's visual cortex functions when it comes to object detection. Because CNN requires a large number of neurons and layers to train data, it is not ideal for small datasets. Obtaining and storing a huge data collection for a scratch program is a challenge. These issues can be solved by using transfer training using a pre-trained data set. This is a dimensionality reduction approach used in deep learning analysis to lower the number of hidden layers and construct neural network applications on tiny data sets with high gain and little information loss. Using transfer learning to retrain a convolutional neural network to categorize a fresh batch of photos, this research investigates visual properties and isolates those that unify the digital image. The developed model satisfied 97% MSE (Mean Squared Error).</dcterms:abstract>
        <dc:date>2022</dc:date>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.semanticscholar.org/paper/Object-Detection-using-Convolutional-Neural-Network-Emmanuel-Onuodu/c61d07cdb15155c9b9fea4af42a3e917f3962a88</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-12-09 11:44:13</dcterms:dateSubmitted>
        <z:libraryCatalog>Semantic Scholar</z:libraryCatalog>
    </rdf:Description>
    <bib:Article rdf:about="http://pen.ius.edu.ba/index.php/pen/article/view/3517">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2303-4521"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Al-Shammary</foaf:surname>
                        <foaf:givenName>Ali Abbas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zaghden</foaf:surname>
                        <foaf:givenName>Nizar</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bouhlel</foaf:surname>
                        <foaf:givenName>Med Salim</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Automatic image annotation system using deep learning method to analyse ambiguous images</dc:title>
        <dcterms:abstract>Image annotation has gotten a lot of attention recently because of how quickly picture data has expanded. Together with image analysis and interpretation, image annotation, which may semantically describe images, has a variety of uses in allied industries including urban planning engineering. Even without big data and image identification technologies, it is challenging to manually analyze a diverse variety of photos. The improvements to the Automated Image Annotation (AIA) label system have been the subject of several scholarly research. The authors will discuss how to use image databases and the AIA system in this essay. The proposed method extracts image features from photos using an improved VGG-19, and then uses nearby features to automatically forecast picture labels. The proposed study accounts for both correlations between labels and images as well as correlations within images. The number of labels is also estimated using a label quantity prediction (LQP) model, which improves label prediction precision. The suggested method addresses automatic annotation methodologies for pixel-level images of unusual things while incorporating supervisory information via interactive spherical skins. The genuine things that were converted into metadata and identified as being connected to pre-existing categories were categorized by the authors using a deep learning approach called a conventional neural network (CNN) - supervised. Certain object monitoring systems strive for a high item detection rate (true-positive), followed by a low availability rate (false-positive). The authors created a KD-tree based on k-nearest neighbors (KNN) to speed up annotating. In order to take into account for the collected image backdrop. The proposed method transforms the conventional two-class object detection problem into a multi-class classification problem, breaking the separated and identical distribution estimations on machine learning methodologies. It is also simple to use because it only requires pixel information and ignores any other supporting elements from various color schemes. The following factors are taken into consideration while comparing the five different AIA approaches: main idea, significant contribution, computational framework, computing speed, and annotation accuracy. A set of publicly accessible photos that serve as standards for assessing AIA methods is also provided, along with a brief description of the four common assessment signs.</dcterms:abstract>
        <bib:pages>176-185</bib:pages>
        <dc:date>2023-04-03</dc:date>
        <z:language>en</z:language>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://pen.ius.edu.ba/index.php/pen/article/view/3517</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-12-09 11:40:27</dcterms:dateSubmitted>
        <z:libraryCatalog>pen.ius.edu.ba</z:libraryCatalog>
        <dc:rights>Copyright (c) 2023 Ali Abbas Al-Shammary, Nizar Zaghden, Med Salim Bouhlel</dc:rights>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2303-4521">
        <dc:title>Periodicals of Engineering and Natural Sciences</dc:title>
        <prism:volume>11</prism:volume>
        <prism:number>2</prism:number>
        <dc:identifier>DOI 10.21533/pen.v11i2.3517</dc:identifier>
        <dc:identifier>ISSN 2303-4521</dc:identifier>
    </bib:Journal>
</rdf:RDF>
